{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "- Tokenization: NLTK, spaCY, **stanfordnlp**\n",
    "- stop word removal: stopwordsiso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp as snlp\n",
    "en = snlp.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/soon/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "en = snlp.Pipeline(lang='en', processors='tokenize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'My name is Cheong Soon Yau. I love eating and running. He was beaten. I\\'m the BEST in the WORLD!!!'\n",
    "tokenized = en(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My\n",
      "name\n",
      "is\n",
      "Cheong\n",
      "Soon\n",
      "Yau\n",
      ".\n",
      "I\n",
      "love\n",
      "eating\n",
      "and\n",
      "running\n",
      ".\n",
      "He\n",
      "was\n",
      "beaten\n",
      ".\n",
      "I\n",
      "'m\n",
      "the\n",
      "BEST\n",
      "in\n",
      "the\n",
      "WORLD\n",
      "!!!\n"
     ]
    }
   ],
   "source": [
    "for sentence in tokenized.sentences:\n",
    "    for token in sentence.tokens:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "'m\n",
      "the\n",
      "BEST\n",
      "in\n",
      "the\n",
      "WORLD\n",
      "!!!\n"
     ]
    }
   ],
   "source": [
    "for token in tokenized.sentences[1].tokens:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgentf2",
   "language": "python",
   "name": "imgentf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
